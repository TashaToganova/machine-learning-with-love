{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в машинное обучение\n",
    "\n",
    "## Семинар #7\n",
    "\n",
    "### Екатерина Кондратьева\n",
    "\n",
    "ekaterina.kondrateva@skoltech.ru\n",
    "\n",
    "code credit @artonson\n",
    "\n",
    "## Обучение без учителя. Поиск аномалий (Anomaly Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кто такие аутлайеры (аномалии)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это наблюдение, которое \"**отличается**\" \"**настолько сильно**\" от \"**прочих наблюдений**\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение аномалий в датасете невозможно без: \n",
    "   1. определения **\"прочих наблюдений\"** (модели данных)\n",
    "   2. определения метрики отклонения (**\"отличия\"**) от **\"прочих наблюдений\"** \n",
    "   3. определения **\"насколько сильно\"** аномальные объекты **\"отличаются\"** от **\"прочих наблюдений\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Назовите самый простой способ детектирования аномалий?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Спойлер: СТД? Квантили?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Нахождение фрода\n",
    "\n",
    "Источник: https://www.kaggle.com/dalpozz/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте проверим, сколько \"фрода\" в этой выборке?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Должно быть  1.7% , что можно сказать про баланс классов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_X = data.drop(columns=\"Class\")\n",
    "full_y = data[\"Class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем делать **красиво**:\n",
    "    \n",
    "    - отложим заранее из оригинальной выборки тестовую, размером 0.4\n",
    "    - будем выбирать модель кросс валидацией на выборке `train`\n",
    "    - не забудем про стратификацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "full_X.reset_index(drop=True)\n",
    "\n",
    "tt_split = train_test_split(full_X, full_y, test_size=0.4, stratify=full_y)\n",
    "\n",
    "train_X, test_X, train_y, test_y = tt_split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем делать **красиво**, выберем несколько метрик:\n",
    "    \n",
    "    1. стандартно `accuracy`, почему это плохо в нашем случае?\n",
    "    2. `precision_score` или доля истинных 1 среди всех помеченных как 1\n",
    "    3. `recall_score` или полнота \"вероятность\", с которой все истинные 1 предсказаны как 1\n",
    "    4. `fbeta_score` - частный случай которого, наш знакомый `f1_score` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, fbeta_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbeta_score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зачем нам $\\beta$ ? : https://en.wikipedia.org/wiki/F1_score\n",
    "\n",
    "Возьмем $\\beta = .17$ как будто, мы заранее знаем, сколько нам ожидать фрода в выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics(y_true, y_pred):\n",
    "    return pd.Series(\n",
    "        {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \n",
    "        \"f1_score\": f1_score(y_true, y_pred),\n",
    "        \n",
    "        \"f_0.2\": fbeta_score(y_true, y_pred, beta=.17),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обнаружение выбросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "\n",
    "def plot_clustering(model, data, size=100):\n",
    "    def _expand(a, b, frac=.5, margin=1.):\n",
    "        return a - abs(a) * frac - margin, b + abs(b) * frac + margin\n",
    "\n",
    "    # Вспомогательная функция для рисования линий уровня и набора точек\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    min_x, min_y = data.min(axis=0)\n",
    "    max_x, max_y = data.max(axis=0)\n",
    "    min_x, max_x = _expand(min_x, max_x)\n",
    "    min_y, max_y = _expand(min_y, max_y)\n",
    "\n",
    "    # создаём регулярную сетку для контуров\n",
    "    all_x = np.linspace(min_x, max_x, num=size)\n",
    "    all_y = np.linspace(min_y, max_y, num=size)\n",
    "    XX, YY = np.meshgrid(all_x, all_y)\n",
    "    test_data = np.c_[XX.ravel(), YY.ravel()]\n",
    "\n",
    "    # опрашиваем предсказания модели\n",
    "    try:\n",
    "        predictions = model.decision_function(test_data).reshape(size, size)\n",
    "        data_scores = model.predict(data)\n",
    "        anomaly_scores = model.decision_function(data)\n",
    "\n",
    "    except AttributeError:\n",
    "        predictions = model._decision_function(test_data).reshape(size, size)\n",
    "        data_scores = model._predict(data)\n",
    "        anomaly_scores = model._decision_function(data)\n",
    "\n",
    "    # создаём график контуров с заливкоц\n",
    "    plt.contourf(all_x, all_y, predictions, cmap=plt.cm.coolwarm)\n",
    "\n",
    "    # отображаем границу принятия решений\n",
    "    threshold = anomaly_scores[data_scores==1.0].min()\n",
    "    plt.contour(XX, YY, predictions, levels=[threshold], linewidths=1)\n",
    "\n",
    "    # нарисуем точки выборки\n",
    "    plt.scatter(data[:, 0], data[:, 1])\n",
    "\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([min_x,max_x])\n",
    "    axes.set_ylim([min_y,max_y])\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём несколько скоплений точек и подмешиваем аномальных точек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "\n",
    "def data_generator(n_samples=100, anomaly_fraction=0.1, n_features=2):\n",
    "    n_anomaly = int(n_samples * anomaly_fraction)\n",
    "    n_normal = n_samples - n_anomaly\n",
    "\n",
    "    normal_data, _ = make_blobs(n_normal, n_features=n_features, centers=3)#какое это распределение?\n",
    "\n",
    "    anomaly_data = np.random.rand(n_anomaly, n_features)\n",
    "\n",
    "    nrm_min = normal_data.min(axis=0).reshape(1, -1)\n",
    "    nrm_ptp = normal_data.ptp(axis=0).reshape(1, -1)\n",
    "    anomaly_data = anomaly_data * nrm_ptp + nrm_min\n",
    "\n",
    "    return np.concatenate([normal_data, anomaly_data], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём несколько скоплений точек, к которым подмешаны аномалии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data_blobs = data_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Эллиптическая Огибающая\n",
    "\n",
    "Метод **Elliptic Envelope** оценивая ковариацию данныхпредполагает, что \n",
    "* данные порожденные эллиптическим распределением\n",
    "\n",
    "* аномальные точки находятся дальше от центра скопления, чем нормальные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = EllipticEnvelope(assume_centered=False, contamination=0.1)\n",
    "\n",
    "model.fit(data_blobs)\n",
    "\n",
    "# \n",
    "plot_clustering(model, data_blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "+ простой в использовании метод\n",
    "+ порождает интерпретируемую границу\n",
    "- годится только для распредений с одним цетром (одномодальных)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на его работу в обнаружении мошенничества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### А как выгладит распредление наших переменых?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EllipticEnvelope()\n",
    "\n",
    "model.fit(train_X)\n",
    "\n",
    "predictions_elliptic = -model.decision_function(test_X)\n",
    "\n",
    "labels_elliptic = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_elliptic = show_metrics(test_y, (labels_elliptic < 0) * 1)\n",
    "metrics_elliptic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Изолирующий Лес (Isolation Forest)\n",
    "\n",
    "* для каждого наблюдения рассчитывается средняя длина пути до него в случайном дереве\n",
    "  * деревья со случайными разбиениями с целью изолировать наблюдения\n",
    "  \n",
    "* чем короче средняя длина пути тем проще описать наблюдение\n",
    "  * тем в более изолированной части пространства оно находится\n",
    "\n",
    "* чем больше разбиений нужно для наблюдения, нормальнее оно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = IsolationForest(n_estimators=100,\n",
    "                        contamination=0.1,\n",
    "                        max_features=1.0,\n",
    "                        max_samples=1.0,\n",
    "                        bootstrap=True,\n",
    "                        random_state=0)\n",
    "model.fit(data_blobs)\n",
    "\n",
    "plot_clustering(model, data_blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Плюсы:\n",
    "\n",
    "* Робастный метод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Минусы:\n",
    "* плохая интерпертация\n",
    "\n",
    "* не различает скопления аномалий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IsolationForest()\n",
    "\n",
    "model.fit(train_X)\n",
    "\n",
    "predictions_isolation = -model.decision_function(test_X)\n",
    "\n",
    "labels_isolation = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_isolation = show_metrics(test_y, (labels_isolation < 0) * 1)\n",
    "metrics_isolation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Local Outlier Factor\n",
    "\n",
    "Основан на наблюдении, что нормальные наблюдения имеют тенденцию **скапливаться**\n",
    "\n",
    "* вводится показатель локальной плотности, обратно пропорциональный средним расстоянием до $k$ ближайших соседей\n",
    "\n",
    "* попарно сравнивается с показателями соседей\n",
    "\n",
    "* вычисляется отношение локальной аномальности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод: https://towardsdatascience.com/local-outlier-factor-for-anomaly-detection-cc0c770d2ebe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = LocalOutlierFactor(n_neighbors=20, \n",
    "                           contamination=0.1,\n",
    "                           metric='minkowski', \n",
    "                           novelty=True,\n",
    "                           p=2)\n",
    "model.fit(data_blobs)\n",
    "\n",
    "plot_clustering(model, data_blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Плюсы:\n",
    "* непараметрический метод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Минусы:\n",
    "* подвержен проблеме \"проклятия размерности\", тк основан на расстояниях\n",
    "* не может отличить скопления аномалий от нормальных точек\n",
    "* без модификаций не поддерживает проверку аномальности на новых данных без обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LocalOutlierFactor()\n",
    "\n",
    "model.fit(train_X)\n",
    "\n",
    "predictions_lof = -model._decision_function(test_X)\n",
    "\n",
    "labels_lof = -predictions_lof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_lof = show_metrics(test_y, (labels_lof < 0) * 1)\n",
    "print( metrics_lof )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One Class SVM\n",
    "\n",
    "Основная идея -- отделить данные в спрямляющем пространстве **мягкой гиперплоскостью** от нуля\n",
    "\n",
    "Решает задачу\n",
    "\\begin{aligned}\n",
    "  & \\underset{\\rho, f\\in \\mathcal{H}}{\\text{минимизировать}}\n",
    "    & & \\tfrac12 \\|f\\|^2 - \\rho\n",
    "        + \\tfrac1{m \\nu} \\sum_{i=1}^m \\max\\bigl\\{\n",
    "            0, \\rho - f(x_i) \\bigr\\}\\,,\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Визуализация одноклассового метода опорных векторов: http://rvlasveld.github.io/blog/2013/07/12/introduction-to-one-class-support-vector-machines/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализация `gamma` для `rbf` ядра: https://bitquill.net/blog/quick-hack-visualizing-rbf-bandwidth/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = OneClassSVM(nu=0.1, kernel='rbf', gamma=0.1)\n",
    "\n",
    "model.fit(data_blobs)\n",
    "\n",
    "plot_clustering(model, data_blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Плюсы:\n",
    "* непараметрический метод\n",
    "* применим не только к объектам из $\\mathbb{R}^n$ (линейного пространства)\n",
    "  * ядра на строках, графах и пр.\n",
    "* может быть полезным при разумном выборе ядра"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Минусы:\n",
    "* вычислительно затратен и плохо масштабируется\n",
    "* необходимо хранить часть обучающей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneClassSVM()\n",
    "\n",
    "model.fit(train_X)\n",
    "\n",
    "predictions_svm = model.decision_function(test_X)\n",
    "\n",
    "labels_svm = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_svm = show_metrics(test_y, (labels_svm < 0) * 1)\n",
    "metrics_svm = show_metrics(test_y, ~(labels_svm < 0) * 1)\n",
    "print(metrics_svm )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бинарная классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся логистической регрессией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "estimator = LogisticRegression(class_weight=None)\n",
    "\n",
    "grid = {\n",
    "    \"C\" : np.logspace(-3, +3, num=10)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем стараться честно построить модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "f1_scorer = make_scorer(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "st_kfold = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим лог-регрессию с кросс валидацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cv_grid = GridSearchCV(estimator, grid, scoring=f1_scorer, cv=st_kfold, n_jobs=1)\n",
    "\n",
    "cv_grid.fit(train_X, train_y) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим метрики для самой лучшей модели по валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_naive = cv_grid.best_estimator_\n",
    "\n",
    "logistic_naive_test_pred = logistic_naive.predict(test_X)\n",
    "\n",
    "metrics_logistic_naive = show_metrics(test_y, logistic_naive_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_logistic_naive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно ли улучшить `precision` и `recall`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Балансировка и Ресэмплинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В дефолтном случае лог-регрессия использует лог-лосс:\n",
    "* на выборке $(x_i, y_i)_{i=1}^m$ с $y_i \\in \\{\\pm 1\\}$ решается задача\n",
    "\\begin{aligned}\n",
    "  & \\underset{\\beta_0, \\beta}{\\text{минимизировать}}\n",
    "    & & \\tfrac12 \\|\\beta\\|_2^2\n",
    "        + C \\sum_{i=1}^m l\\bigl(y_i, f(x_i)\\bigr)\n",
    "        \\,, \\\\\n",
    "  & & & l(y, p) = \\log \\bigl(1 + \\exp\\{- y p \\}\\bigr)\n",
    "        \\,, \\\\\n",
    "  & & & f(x) = x^{\\rm T} \\beta + \\beta_0\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но можно минимизировать функцию потерь, взвешенный по меткам класса:\n",
    "\\begin{aligned}\n",
    "  & \\underset{\\beta_0, \\beta}{\\text{минимизировать}}\n",
    "    & & \\tfrac12 \\|\\beta\\|_2^2\n",
    "        + C w_+ \\sum_{i\\colon y_i = +1} l\\bigl(+1, f(x_i)\\bigr)\n",
    "        + C w_- \\sum_{i\\colon y_i = -1} l\\bigl(-1, f(x_i)\\bigr)\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* в случае \"наивного\" взвешивания $w_+ = w_- = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* в случае \"сбалансированного\" взвешивания $w_+ = \\tfrac{m}{2 n_+}$ и $w_- = \\tfrac{m}{2 n_-}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LogisticRegression(class_weight = \"balanced\")\n",
    "\n",
    "cv_grid = GridSearchCV(estimator, grid, scoring=f1_scorer, cv = st_kfold, n_jobs = 1)\n",
    "\n",
    "cv_grid.fit(train_X, train_y) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_balanced = cv_grid.best_estimator_\n",
    "\n",
    "logistic_balanced_test_pred = logistic_balanced.predict(test_X)\n",
    "\n",
    "metrics_logistic_balanced = show_metrics(test_y, logistic_balanced_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_logistic_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ресэмплинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная идея - сбалансировать классы в обучающей выборке добавлением наблюдений или изменением их веса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import check_random_state, safe_indexing\n",
    "\n",
    "\n",
    "def undersample(X, y, ratio=20, pos_label=1, random_state=None):\n",
    "    random_state = check_random_state(random_state)\n",
    "\n",
    "    # отбрасываем случайную долю наблюдений доминирующего класса\n",
    "    class_major_index = np.flatnonzero(y != pos_label)\n",
    "\n",
    "    n_major = int(len(class_major_index) / ratio)\n",
    "    class_major_index = random_state.permutation(class_major_index)\n",
    "    class_major_index = class_major_index[:n_major]\n",
    "\n",
    "    # выбираем все примеры минорного класса\n",
    "    class_minor_index = np.flatnonzero(y == pos_label)\n",
    "\n",
    "    # составляем новую (временную) обучающую выборку\n",
    "    indices = np.r_[class_major_index, class_minor_index]\n",
    "\n",
    "    return safe_indexing(X, indices), safe_indexing(y, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прореживание доминирующего класса\n",
    "* может сильно уменьшить количество наблюдений в итоговой обучающей\n",
    "выборке, что негативно сказывается на обученном классификаторе:\n",
    "    * может плохо обобщить из-за нехватки данных;\n",
    "    * сам алгоритм обучения моджет очень чувствителен к изменению выборки.\n",
    "\n",
    "\n",
    "* снижает затраты на обучение модели, позволяя тренировать больше классификаторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LogisticRegression(class_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим из текущего трейна валидацоннный набор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_X, val_X, dev_y, val_y = train_test_split(train_X, train_y, test_size=0.25,\n",
    "                                              stratify=train_y, random_state=321)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведём валидацию модели с андерсэмплингом вручную"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.base import clone\n",
    "\n",
    "n_resamples, ratio = 7, 10\n",
    "par_grid = ParameterGrid(grid)\n",
    "\n",
    "results_grid = []\n",
    "for par in par_grid:\n",
    "\n",
    "    results_resample = []\n",
    "    for b in range(n_resamples):\n",
    "        und_X, und_y = undersample(dev_X, dev_y, ratio, pos_label=1, random_state=None)\n",
    "\n",
    "        cv_estimator = clone(estimator).set_params(**par)\n",
    "        cv_estimator.fit(und_X, und_y)\n",
    "        \n",
    "        cv_val_pred = cv_estimator.predict(val_X)\n",
    "        results_resample.append(show_metrics(val_y, cv_val_pred))\n",
    "\n",
    "    results_resample = pd.concat(results_resample, axis=1).T\n",
    "\n",
    "    results_grid.append((par, results_resample.mean().rename(\"mean\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбираем $F_1$ метрики и находим наилучшую модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = [(par, met[\"f1_score\"]) for par, met in results_grid]\n",
    "\n",
    "best_par_, _ = f1_scores[np.argmax([f1 for par, f1 in f1_scores])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем андерсэмплинг и обучаем модель заново"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "und_X, und_y = undersample(train_X, train_y, ratio, pos_label=1, random_state=None)\n",
    "\n",
    "logistic_undersample = clone(estimator).set_params(**best_par_)\n",
    "logistic_undersample.fit(und_X, und_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_undersample_test_pred = logistic_undersample.predict(test_X)\n",
    "\n",
    "metrics_logistic_undersample = show_metrics(test_y, logistic_undersample_test_pred)\n",
    "print(metrics_logistic_undersample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = pd.concat(dict([\n",
    "    (\"elliptic\", metrics_elliptic),\n",
    "    (\"isolation\", metrics_isolation),\n",
    "    (\"svm\", metrics_svm),\n",
    "    (\"logistic_naive\", metrics_logistic_naive),\n",
    "    (\"logistic_balanced\", metrics_logistic_balanced),\n",
    "    (\"logistic_undersample\", metrics_logistic_undersample)\n",
    "]), axis=1)\n",
    "\n",
    "all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
